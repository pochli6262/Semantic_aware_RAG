# Semantic-Aware RAG for Competitive Programming Repair

> **Author:** Liao Po-Chun
> Department of Computer Science and Information Engineering, National Taiwan University
> Undergraduate Research Project (2025)

---

## ⚙️ Quick Start

### 🪜 Step 1: Set up the Environment

It is recommended to use Conda or venv:

```bash
conda create -n semanticrag python=3.10
conda activate semanticrag
```

---

### 🪜 Step 2: Configure OpenAI API Key

This project uses GPT models (e.g., GPT-4o-mini / GPT-5-mini). You must set your OpenAI API key before running.

macOS / Linux:

```bash
export OPENAI_API_KEY="your_api_key_here"
```

Windows (PowerShell):

```bash
setx OPENAI_API_KEY "your_api_key_here"
```

> ⚠️ Without this, you will encounter `openai.error.AuthenticationError` or "invalid API key".

---

### 🪜 Step 3: Install Dependencies

Create a `requirements.txt` file with:

```txt
faiss-cpu
numpy
tqdm
transformers
openai
torch
pandas
```

Then install:

```bash
pip install -r requirements.txt
```

> 💡 For GPU environments, use `faiss-gpu` and `torch==2.x` instead.

---

## 🧩 Project Structure

| File                | Description                                                                          |
| ------------------- | ------------------------------------------------------------------------------------ |
| `mode1.py`          | Code-only Retrieval: uses only code embeddings for search                            |
| `mode2.py`          | Problem-guided Retrieval: guided by problem text, code embedding only                |
| `mode3.py`          | Semantic RAG – Joint Embedding: combines problem and code into one vector            |
| `mode456.py`        | Semantic RAG – Dual Embedding: embeds problem and code separately (best performance) |
| `rag_judge_stat.py` | Main script for evaluating results (SA / TPR / Precision@5)                          |

---

## 🚀 Running the Project

### 🔹 Individual Modes (mode1.py ~ mode3.py)

```bash
python mode1.py    # Code-only retrieval
python mode2.py    # Problem-guided retrieval
python mode3.py    # Joint embedding
```

---

### 🔹 Multi-Mode Control (mode456.py)

`mode456.py` supports multiple retrieval strategies controlled via the environment variable `RETRIEVAL_MODE`:

| RETRIEVAL_MODE | Mode Name                | Description                                            |
| -------------- | ------------------------ | ------------------------------------------------------ |
| 2              | Problem-guided Retrieval | Uses problem text to guide retrieval                   |
| 3              | Joint Embedding          | Merges problem and code into a single vector           |
| 4              | Dual Embedding           | Separate embeddings for problem and code (recommended) |

macOS / Linux:

```bash
RETRIEVAL_MODE=2 python3 mode456.py
RETRIEVAL_MODE=3 python3 mode456.py
RETRIEVAL_MODE=4 python3 mode456.py   # Recommended
```

Windows (PowerShell):

```bash
set RETRIEVAL_MODE=4
python mode456.py
```

> 💡 Default mode is typically Dual Embedding if not specified.

---

### 🔹 Evaluation & Statistics

Run:

```bash
python rag_judge_stat.py
```

This script computes and displays:

* **Strict Accuracy (SA)**
* **Test Pass Rate (TPR)**
* **Precision@5**

Outputs are displayed in terminal and optionally written to `results_summary.csv`.

---

## 📊 Example Output

```
=== GPT-5-mini ===
Code-only Retrieval: SA = 52.5%
Semantic RAG (Dual Embedding): SA = 83.5%
Precision@5 = 0.384
```

---

## 📁 Output Files

| File                 | Description                           |
| -------------------- | ------------------------------------- |
| `results_modeX.json` | Repair results for each test case     |
| `retrieval_log.csv`  | Retrieval history and distance scores |
| `repair_summary.txt` | Summary of success/failure results    |

---

## 🧠 Project Overview

This project introduces a **Semantic-Aware Retrieval-Augmented Generation (RAG)** framework for **automated single-line error repair** in competitive programming.
Unlike traditional Automated Program Repair (APR), this method combines **problem descriptions** and **buggy code embeddings** to guide retrieval and enhance repair accuracy.

---

## 🏗️ System Architecture

The framework operates in five main stages:

1. **Bug Summary Generation** – Generated by an oracle or LLM.
2. **Retrieval** – Problem descriptions and buggy code are embedded into a FAISS index.
3. **RAG Augmentation** – Retrieves the top-k most relevant repair examples.
4. **Repair Generation** – LLM generates fixes based on code and retrieved examples.
5. **Validation** – Candidate fixes are tested against problem-specific cases.

---

## 🧪 Experimental Results

| Model       | Mode           | Strict Accuracy | Precision@5 |
| ----------- | -------------- | --------------- | ----------- |
| GPT-4o-mini | Zeroshot       | 14.5%           | –           |
| GPT-4o-mini | Dual Embedding | 41.5%           | 0.314       |
| GPT-5-mini  | Zeroshot       | 48.5%           | –           |
| GPT-5-mini  | Dual Embedding | **83.5%**       | **0.384**   |

Dual Embedding significantly boosts retrieval precision and repair success rate, outperforming zeroshot by nearly 60%.

---

## 🔍 Case Study

In one ConDefects example, the buggy code failed to enforce strict superiority conditions.

* **Code-only retrieval** returned irrelevant operator adjustments.
* **Dual Embedding** retrieved examples with stricter condition checks, guiding the model to correctly fix the logic and pass all test cases.

---

## 🧩 Dataset

**ConDefects Dataset**

* ~200 problems and 3,800 test cases
* Each sample includes:

  * `faultyVersion.py`
  * `correctVersion.py`
  * `faultyLocation.txt`
* 2,519 used for retrieval; 200 for evaluation

---

## 📈 Future Work

* Improve **automatic bug summary generation** for better retrieval quality.
* Extend to **multi-line and multi-file repairs**.
* Explore additional code representation models to enhance semantic precision.

